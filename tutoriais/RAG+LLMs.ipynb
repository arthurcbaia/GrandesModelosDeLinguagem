{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDBNqBVxcQCa"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms import AzureOpenAI\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, get_response_synthesizer, set_global_service_context\n",
        "from llama_index.retrievers import VectorIndexRetriever\n",
        "from llama_index.query_engine import RetrieverQueryEngine\n",
        "from llama_index.embeddings import HuggingFaceEmbedding\n",
        "import os\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "T8-OmfcTftuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM\n",
        "Nesse caso, utlizando o modelo [GPT 3.5 com janela de contexto de 16 mil tokens](https://platform.openai.com/docs/models/gpt-3-5). Mais especificamente, utlizando um com [envolcro do serviço da Azure](https://docs.llamaindex.ai/en/stable/api_reference/llms/azure_openai.html) feito pelo framework LLamaIndex."
      ],
      "metadata": {
        "id": "gJNAWaz6s0pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "azure_endpoint = userdata.get('OPENAI_API_BASE')\n",
        "api_version = \"2023-03-15-preview\"\n",
        "\n",
        "llm = AzureOpenAI(\n",
        "    model=\"gpt-35-turbo-16k\",\n",
        "    deployment_name=\"gpt-16k\",\n",
        "    api_key=api_key,\n",
        "    azure_endpoint=azure_endpoint,\n",
        "    api_version=api_version,\n",
        ")"
      ],
      "metadata": {
        "id": "8l4fHEiBjX4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embed Model\n",
        "Os modelos de *embed* são utilizados para representar documentos e consultas em uma forma numérica que captura a semântica do texto. Esses *embeddings* são empregados durante a construção do índice e ao realizar consultas usando o mecanismo de consulta. O modelo de *embeddings* é essencial, pois permite ao sistema entender e comparar o significado de diferentes textos. Neste contexto, estamos utilizando o modelo [e5]( https://huggingface.co/intfloat/multilingual-e5-smal), que é considerado o melhor modelo multilíngue de código aberto para a tarefa de recuperação de informação semântica, de acordo com o ranking [MTEB](https://huggingface.co/spaces/mteb/leaderboard), que mede quão bons são os modelos de para a tarefa de busca. Além disso, utilizamos o invólucro do LlamaIndex em combinação com o hub HuggingFace para essa finalidade.\n"
      ],
      "metadata": {
        "id": "1efZetQPs2Vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = HuggingFaceEmbedding(model_name=\"intfloat/multilingual-e5-small\")"
      ],
      "metadata": {
        "id": "1DJ28sGNmyaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "O código abaixo está definindo um contexto de serviço global no LlamaIndex. O ServiceContext é uma classe que agrupa vários componentes que são usados em conjunto para realizar tarefas no LlamaIndex. Isso inclui o modelo de linguagem grande (LLM), o modelo de *embeddings*, e outros componentes. A função from_defaults é usada para criar um ServiceContext com valores padrão para todos os componentes, mas permite que você substitua alguns deles, como o llm e o embed_model neste caso."
      ],
      "metadata": {
        "id": "Q4oiW8y4tEs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "service_context = ServiceContext.from_defaults(\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")\n",
        "set_global_service_context(service_context)"
      ],
      "metadata": {
        "id": "Yoz5GuaqjrKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ingestão de dados\n",
        "Como citado, estamos utilizando dados referentes a pós-graduação do Instituto de Computação da Unicamp. Mais especificamente, sobre a [pagina de defesa do mestrado e doutorado](https://ic.unicamp.br/pos-graduacao/informacao-para-docentes-e-pos-graduandos/vida-academica-da-pos-graduacao/procedimentos-e-formularios-para-defesa-de-mestrado-e-doutorado/).\n",
        "\n",
        "O framework utlizado permite a [ingestão de dados diretamente de um diretório](https://gpt-index.readthedocs.io/en/latest/examples/data_connectors/simple_directory_reader.html), nesse caso /data. Com a designação inicial, é possível apontar o arquivo que sera utlizado para a criação da base de *embeddings*. A partir dele, é possível criar um [index](https://docs.llamaindex.ai/en/stable/api_reference/indices/vector_store.html), que consegue indexar as informações contidas dento do arquivo.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W7h4fYyNtHhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(\n",
        "    input_files=[\"./data/ic.txt\"]\n",
        ").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n"
      ],
      "metadata": {
        "id": "XtipYVx9chnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuração do *Retriever*\n",
        "O [VectorIndexRetriever]( https://docs.llamaindex.ai/en/stable/api_reference/query/retrievers/vector_store.html) no LlamaIndex é um componente que usa um modelo de linguagem grande (LLM) para definir automaticamente os parâmetros de consulta do armazenamento de vetores. Ele recebe dois parâmetros principais: index e similarity_top_k. O parâmetro index é o índice do armazenamento de vetores que o Retriever irá consultar. O parâmetro similarity_top_k determina o número de resultados mais semelhantes a serem retornados pela consulta. Em outras palavras, o VectorIndexRetriever é uma ferramenta que ajuda a encontrar as informações mais relevantes em um conjunto de dados, com base na semelhança com a consulta fornecida.\n",
        "\n"
      ],
      "metadata": {
        "id": "fFiKiXrG2V3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# configure retriever\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=2,\n",
        ")"
      ],
      "metadata": {
        "id": "XkqQR3Myz_rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Escolher tipo de resposta gerada pela LLM\n",
        "O [get_response_synthesizer](https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/response_modes.html) no LlamaIndex é uma função que gera uma resposta a partir de um Modelo de Linguagem Grande (LLM), usando uma consulta do usuário e um conjunto de trechos de texto. O resultado é um objeto de resposta. O método para fazer isso pode assumir muitas formas, desde algo simples como iterar sobre trechos de texto, até algo complexo como construir uma árvore. A ideia principal é simplificar o processo de gerar uma resposta usando um LLM em seus dados.\n",
        "\n"
      ],
      "metadata": {
        "id": "HLPhnENN0q9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_synthesizer = get_response_synthesizer(\n",
        "    response_mode=\"simple_summarize\",\n",
        ")"
      ],
      "metadata": {
        "id": "PuTS3nLC0F_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Motor de busca e criação de resposta\n",
        "O [RetrieverQueryEngine](https://gpt-index.readthedocs.io/en/latest/examples/query_engine/CustomRetrievers.html) é um componente do LlamaIndex que permite combinar uma consulta em linguagem natural com a recuperação de uma resposta relevante. Ele depende de um \"retriever\" para recuperar informações de um índice, que pode ser um arquivo de texto, e de um \"response_synthesizer\" como parâmetro, que ajusta a forma como a resposta é criada. Este componente principal é responsável por processar a consulta do usuário e gerar uma resposta apropriada com base nas informações recuperadas pelo retriever\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oSGNzmp200nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever=retriever,\n",
        "    response_synthesizer=response_synthesizer,\n",
        ")"
      ],
      "metadata": {
        "id": "pyWod3zF0SQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1"
      ],
      "metadata": {
        "id": "0GKscTWNsAJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Quantos membros deve ter uma banca de defesa de dissertação?\"\n",
        "query_engine = index.as_query_engine()\n",
        "answer = query_engine.query(query)\n",
        "\n",
        "print(answer.get_formatted_sources())\n",
        "print(\"query was:\", query)\n",
        "print(\"answer was:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4rRBqvzl_7x",
        "outputId": "bd9394ed-e2c3-49f6-83c6-f368842c531f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Source (Doc id: 25bffb9a-7c99-40ea-8bb1-c885fe24f1ea): O processo de defesa de Mestrado e de Doutorado é a conclusão da trajetória do aluno no curso.\n",
            "To...\n",
            "\n",
            "> Source (Doc id: fa1edc89-b6a4-4d59-bb85-950f456ca710): Este arquivo deverá ser impresso, assinado por você e seu/sua orientador(a) e entregue à Secretar...\n",
            "query was: Quantos membros deve ter uma banca de defesa de dissertação?\n",
            "answer was: Uma banca de defesa de dissertação deve ter pelo menos três membros.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2"
      ],
      "metadata": {
        "id": "ayZ5JcZBsJUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Quais passos devo seguir para defender minha tese ou dissertação?\"\n",
        "query_engine = index.as_query_engine()\n",
        "answer = query_engine.query(query)\n",
        "\n",
        "print(\"answer was:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYqXgCcdr-9a",
        "outputId": "fcd1756c-22e1-4a92-9724-1877ec6aa9f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "answer was: Para defender sua tese ou dissertação, você deve seguir os seguintes passos:\n",
            "\n",
            "1. Certifique-se de ter cumprido todos os requisitos do Programa de Pós-Graduação.\n",
            "2. Verifique se todas as coorientações estão devidamente registradas no sistema SIGA.\n",
            "3. Solicite a verificação completa de sua situação acadêmica enviando um e-mail para a Secretaria de Pós-Graduação.\n",
            "4. Preencha o Formulário de Publicação (para alunos de Doutorado) ou o Formulário de Formato Alternativo (para alunos que apresentem o trabalho no formato alternativo) e entregue na Secretaria de Pós-Graduação.\n",
            "5. Envie o Relatório de Verificação de Escrita Original conforme as orientações da Instrução Normativa da CPG 01/2021.\n",
            "6. Verifique os prazos de solicitação de defesa e preencha as informações da defesa nos formulários online.\n",
            "7. Suba um arquivo PDF com o texto da dissertação ou tese no sistema.\n",
            "8. Durante a defesa, os membros da Comissão Examinadora que participarão de forma remota receberão orientações sobre a videoconferência.\n",
            "9. Após a defesa, providencie a Ficha Catalográfica e faça o upload da versão final da dissertação/tese no SIGA.\n",
            "10. Verifique se o trabalho está formatado de acordo com o padrão da Unicamp e envie o arquivo PDF para verificação da formatação pela Secretaria de Pós-Graduação.\n",
            "11. Após a confirmação de que está tudo correto, faça o upload da versão final no SIGA.\n",
            "12. Cumpra os requisitos específicos de agradecimento e citação da bolsa recebida, se aplicável.\n",
            "13. Caso queira exemplares impressos, entre em contato diretamente com a Gráfica da Unicamp para solicitar orçamento e informações sobre pagamento e retirada.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questão 3"
      ],
      "metadata": {
        "id": "m8v8RPHPt0_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"O orientador pode falar algo durante a apresentação?\"\n",
        "query_engine = index.as_query_engine()\n",
        "answer = query_engine.query(query)\n",
        "\n",
        "print(answer.get_formatted_sources())\n",
        "print(\"query was:\", query)\n",
        "print(\"answer was:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhfBrrKLt0Nx",
        "outputId": "98a10978-ac9a-4d30-dd0c-91dfcad2972d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Source (Doc id: ae395a17-7945-41a4-b62d-555b10ca800c): A partir de 01/05/2022 as defesas devem ocorrer de forma presencial, porém recomenda-se que a Com...\n",
            "\n",
            "> Source (Doc id: 4974aff1-f9c5-4e19-bd03-f0f6219e1a74): Prazo mínimo de solicitação\n",
            "Deve solicitar religamento?\n",
            "Mestrado\n",
            "Aluno ativo\n",
            "45 dias\n",
            "Não\n",
            "Aluno de...\n",
            "query was: O orientador pode falar algo durante a apresentação?\n",
            "answer was: Yes, the advisor can speak during the presentation.\n"
          ]
        }
      ]
    }
  ]
}